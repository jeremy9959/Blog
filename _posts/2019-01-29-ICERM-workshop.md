---
layout: single
title: ICERM workshop on scientific machine learning
excerpt: Some highlights from the workshop 1/28/2019-1/30/2019
tags: 
  - mathematics
  - machine-learning
---

## Some highlights from the January, 2019 ICERM Workshop on Scientific Machine Learning

[Here is the workshop home page](https://icerm.brown.edu/events/ht19-1-sml/).

A general theme of this meeting was that neural networks give rise to a certain class of functions, and one can
try to approximate general functions by this restricted class.  A considerable number of talks talked about using
neural networks as a tool to approximate solutions to various types of dynamical systems.

### Hanin's talk on likelihood of exploding gradients in RelU networks.

This talk uses random matrix theory to prove a result that describes
how increasing the depth/width of a neural network influences the
likelihood of "exploding or vanishing gradients."

See [Boris Hanin](http://www.math.tamu.edu/~bhanin/),  [Which Neural Net Architectures Give Rise to Exploding and Vanishing Gradients](https://arxiv.org/abs/1801.03744).

### Petersen's talk on function theoretic properties of functions represented by Neural Networks.

This talk shows that the class of functions computed by neural networks, viewed inside the standard Banach spaces like $L^{p}$, does not have good properties.  

See [Philipp Petersen](https://www.maths.ox.ac.uk/people/philipp.petersen), [Topological properties of the set of functions generated by neural networks of fixed size](https://arxiv.org/abs/1806.08459).

### Carlberg's talk on reduction of order in numerical PDE's using machine learning.

This talk explained how ideas from neural networks could be applied to improve very large scale computational
solutions to PDE's.  It was hard to follow in detail but very impressive.

See the many papers of [Kevin Carlberg](https://www.sandia.gov/~ktcarlb/).

### Le Song's talk on ODE methods for sequential Bayesian analysis

See [Le Song](https://www.cc.gatech.edu/~lsong/index.html). 
